title: 深度学习-NeLet5
date: 2016-03-27 14:48:41
tags: [DL]
categories: DL
---


# LeNet5网络结构

from [zouxy09][1], [tornadomeet][2]

![LeNet][3]
 1. C1层由6张`28*28`大小的特征图构成，由用6个`5*5`大小的patch对`32*32`大小的输入图进行卷积得到，`28=32-(5-1)`，其中每次移动间隔为1个像素。
 
 2. S2层由C1层pooling得到，对每四个像素进行一次pooling，结果为6张`14*14`。
 
 3. C3层将S2层的特征图用1个输入层为150（=`5*5*6`，不是`5*5`）个节点，输出层为16个节点的网络进行卷积。

----------

# S2--->C3

![LeNet2][4]


 1. 首先把网络150-16（表面输入层节点为150，隐含层节点为16）中输入的150个节点分成6个部分，每个部分为连续的25个节点。取出倒数第3个部分的节点（为25个），且同时是与隐含层16个节点中的第4（因为对应的是3号，从0开始计数的）个相连的那25个值，reshape为`5*5`大小，用这个`5*5`大小的特征patch去卷积S2网络中的倒数第3个特征图，假设得到的结果特征图为h1。
 
 2. 同理，取出网络150-16中输入的倒数第2个部分的节点（为25个），且同时是与隐含层16个节点中的第5个相连的那25个值，reshape为`5*5`大小，用这个`5*5`大小的特征patch去卷积S2网络中的倒数第2个特征图，假设得到的结果特征图为h2。

 3. 继续，取出网络150-16中输入的最后1个部分的节点（为25个），且同时是与隐含层16个节点中的第5个相连的那25个值，reshape为`5*5`大小，用这个`5*5`大小的特征patch去卷积S2网络中的最后1个特征图，假设得到的结果特征图为h3。

 4. 最后将h1，h2，h3这3个矩阵相加得到新矩阵h，并且对h中每个元素加上一个偏移量b，且通过sigmoid的激发函数，即可得到C3中的特征图H3。


----------
# C3

C3中每个特征图由S2中所有6个或者几个特征map组合而成。为什么不把S2中的每个特征图连接到每个C3的特征图呢？原因有2点。

> 第一，不完全的连接机制将连接的数量保持在合理的范围内。
> 第二，也是最重要的，其破坏了网络的对称性。由于不同的特征图有不同的输入，所以迫使他们抽取不同的特征（希望是互补的）


  [1]: http://blog.csdn.net/zouxy09/article/details/8781543
  [2]: http://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html#!comments
  [3]: http://7xlbd9.com1.z0.glb.clouddn.com/hsh_blog_LeNet.png
  [4]: http://7xlbd9.com1.z0.glb.clouddn.com/hsh_blog_LeNet2.png