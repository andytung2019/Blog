title: 机器学习-分类方法总结
date: 2015-08-21 15:47:20
tags: [classification,Machine-Learning]
categories: Machine-Learning
---

------

> * KNN
> * 决策树
> * 朴素贝叶斯-基于概率论的分类方法
> * Logistic回归
> * SVM
> * AdaBoost
> * 分类性能度量指标

------
## KNN

### 1.优缺点

* 优点：进度较高，对异常值不敏感，无数据输入假定。
* 缺点：计算复杂度高，空间复杂度高。
* 适用数据范围：数值型和标称型。

### 2. 基本理论
训练集：存在且对应标有标签
测试集：输入的无标签数据
1. 将测试数据的特征与训练集中所有的样本进行比较
2. 然后提取样本中与之最相似的**前k个**样本数据
3. 统计**前k个**样本中出现次数最多的分类作为测试数据的类别

## 决策树
### 优缺点
* 优点：计算复杂度不高，结果易理解，对中间值缺失不敏感，可以处理不相关的特征数据。
* 缺点：可能会过度匹配。
* 适用数据范围：数值型（离散化）和标称型。

### 基本理论
1. 评估样本数据中每个特征，选择出决定性作用的特征进行分类
2. 在所有分支中，正确的数据分类（分支）则无需再进行划分，若分支中的数据仍然不属于同一种类型，则需要重新再分支内进行划分，直到所有具有相同类型的数据均在一个数据子集下。
```
createBranch()
{
    if so return 类标签;
    else
        寻找划分最优特征
        划分数据
        创建分支节点
            for 每个划分子集
                createBranch();//创建分支并返回结果到分支节点中
        return 分支节点
}
```

## 朴素贝叶斯-基于概率论的分类方法
    
### 优缺点

* 优点：在数据较少的情况下依然有效，可以处理多类别问题
* 缺点：对输入数据的准备方式较为敏感（？）
* 适用数据范围：标称型数据

### 基本理论

核心思想：选择具有最高概率的决策。
朴素贝叶斯的两种假设：
- 特征之间相互统计独立（naive）。
- 每个特征同等重要。

根据条件概率公式：
$$P(A|B)=\frac{P(AB)}{P(B)}$$
也即有**贝叶斯定理**：
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
- 假设有待分类数据$x={a_1,a_2,...,a_m}$，其中$a_m$为$x$的一个特征；
- 再假设有已知类别标签的训练样本集合：$T={b_1,b_2,...,b_n}$；
- 计算$x$属于每一类的概率$P(b_1|x),P(b_2|x),...,P(b_n|x)$；
- 若有$P(b_k|x)=max(P(b_1|x),P(b_2|x),...,P(b_n|x))$，则可以说$x$属于第$k$类。
- 计算$P(b_1|x),P(b_2|x),...,P(b_n|x)$的概率:
$$P(a_1|b_1),P(a_2|b_1),...,P(a_m|b_1);...;P(a_1|b_n),P(a_2|b_n),...,P(a_m|b_n)$$
根据朴素贝叶斯的假设：特征之间相互独立，则有：
$$P(x|b_j)=P(a_1|b_j)P(a_2|b_j)...P(a_m|b_j)=\prod_{i=1}^{m}P(a_i|b_j)$$
则：
$$P(b_j|x)=\frac{P(x|b_j)P(b_j)}{P(x)} = \frac{\prod_{i=1}^mP(a_i|b_j)P(b_j)}{P(x)}$$

### 基本阶段
* 准备工作阶段：确定特征属性，形成训练样本集合。
* 分类器训练阶段：生成分类器，计算每个类别在样本中出现的频率以及每个特征划分飞每个类别条件概率的估计。
* 应用阶段：使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。

## Logistic回归
### 优缺点

优点：计算代价不高，易于理解和实现
缺点：容易欠拟合，分类精度可能不高
适用数据范围：标称型和数值型数据

### 基本理论
**期望函数能够接受所有输入然后预测类别**，对于一个二分类，可以选择`Sigmoid`函数：
$$\sigma(z)=\frac{1}{1+e^{-z}}$$
我们可以在每个特征上分别乘以一个回归系数再相加：
$$z=w_{0}·x_{0}+w_{1}·x_{1}+...+w_{n}·x_{n}$$
通过最优化算法利用**训练样本**来获得最优回归系数。

## SVM
优缺点：
优点：泛化错误率低，计算开销小
缺点：对数据调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。
适用数据范围：标称型和数值型数据。


## AdaBoost
优缺点：
优点：泛化错误率低，易编码，可以应用在大部分分类器上，无需参数的调整。
缺点：对离散点敏感。
适用数据范围：标称型和数值型数据。
## 基本理论
### bagging与boosting
 * bagging技术（bootstrap aggregating）中，实在原始数据集中选择S次后得到S个新数据集的技术。新数据集和原始数据集的大小相等。每一个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。它允许多次选择同一个样本，新的数据集中可以有重复的值，原始数据中某些值则可能不在其中。
 在S个新数据集中分类作用某种算法得到S个分类器，对于新数据便可以利用这S个分类器进行分类，最后选择分类器投票结果最多的作为最后的结果。
 * boosting中不同分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练，通过关注被已有分类器错分的那些数据来获得新的分类器。boosting中分类器的权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。
### Adaboost
* 训练数据中的每一个样本，并赋予这些一个权重，构成向量$D$，权重初始化为相同的值。
* 先在训练数据上训练出一个弱分类器并计算其错误率，然后在同一个数据集上在此训练弱分类器，在第二次训练中将会调整每个样本的权重，其中第一次分对的样本的权重降低，分错的权重将会提高。
* Adaboost为每个分类器分配了一个权重值alpha，它是基于每个弱分类器的错误率$\epsilon$而计算的：
$$\epsilon=\frac{未正确分类的样本}{所有的样本} $$
$$\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})$$
* 计算出alpha的值之后可以对权重向量$D$进行更新：
若样本分类正确：
$$D_{i}^{i+1}=\frac{D_{i}^{i}e^{-\alpha}}{sum(D)}$$
若样本分类错误：
$$D_{i}^{i+1}=\frac{D_{i}^{i}e^{\alpha}}{sum(D)}$$
不断迭代直到错误率为零或弱分类器次数达到用户指定值。

## 分类性能度量指标

